{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jaxify\n",
    "\n",
    "> The core functionality of `javiche`. Defines the `@jaxit()` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp jaxify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "from ceviche import jacobian\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import List\n",
    "from functools import lru_cache, wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# from https://gist.github.com/Susensio/61f4fee01150caaac1e10fc5f005eb75\n",
    "def np_cache(*args, **kwargs): \n",
    "    \"\"\"LRU cache implementation for functions whose FIRST parameter is a numpy array\n",
    "    >>> array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    >>> @np_cache(maxsize=256)\n",
    "    ... def multiply(array, factor):\n",
    "    ...     print(\"Calculating...\")\n",
    "    ...     return factor*array\n",
    "    >>> multiply(array, 2)\n",
    "    Calculating...\n",
    "    array([[ 2,  4,  6],\n",
    "           [ 8, 10, 12]])\n",
    "    >>> multiply(array, 2)\n",
    "    array([[ 2,  4,  6],\n",
    "           [ 8, 10, 12]])\n",
    "    >>> multiply.cache_info()\n",
    "    CacheInfo(hits=1, misses=1, maxsize=256, currsize=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    def decorator(function):\n",
    "        @wraps(function)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            mod_args = []\n",
    "            for i,arg in enumerate(args): # modified to allow arbitrary amounts of arrays\n",
    "              if isinstance(arg, np.ndarray) or isinstance(arg, jax.Array):\n",
    "                mod_args.append(array_to_tuple(arg))\n",
    "              else:\n",
    "                mod_args.append(arg)\n",
    "            return cached_wrapper(*mod_args, **kwargs)\n",
    "\n",
    "        @lru_cache(*args, **kwargs)\n",
    "        def cached_wrapper(hashable_array, *args, **kwargs):\n",
    "            array = np.array(hashable_array)\n",
    "            return function(array, *args, **kwargs)\n",
    "\n",
    "        def array_to_tuple(np_array):\n",
    "            \"\"\"Iterates recursivelly.\"\"\"\n",
    "            #print(type(np_array))\n",
    "            if isinstance(np_array, jax.Array):\n",
    "              np_array = np.asarray(np_array)\n",
    "            try:\n",
    "              return tuple(array_to_tuple(_) for _ in np_array)\n",
    "            except TypeError:\n",
    "              return np_array\n",
    "\n",
    "        # copy lru_cache attributes over too\n",
    "        wrapper.cache_info = cached_wrapper.cache_info\n",
    "        wrapper.cache_clear = cached_wrapper.cache_clear\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def jaxit(\n",
    "  mode: str='reverse', #the mode used to calculate the jacobian using `ceviche`\n",
    "  argnums: List[int]=[0], #the argument indices this function should be differentiable against\n",
    "  cache: bool = False\n",
    "  ):\n",
    "  \"\"\"\n",
    "    make a function that internally uses autograd compatible to jax gradient calculations\n",
    "\n",
    "    Attention: only a single output variable is supported\n",
    "  \"\"\"\n",
    "  def identity_decorator(fn):\n",
    "    return fn\n",
    "  \n",
    "  caching_decorator = identity_decorator\n",
    "  if cache:\n",
    "    caching_decorator = np_cache()\n",
    "\n",
    "  def inner(function):\n",
    "    grad_fns = [jacobian(function, mode=mode, argnum=i) for i in argnums]\n",
    "\n",
    "    @jax.custom_jvp\n",
    "    @caching_decorator\n",
    "    def jaxed(*args):\n",
    "      return function(*args)\n",
    "    \n",
    "    @caching_decorator\n",
    "    @jaxed.defjvp\n",
    "    def jaxed_jvp(primals, tangents):\n",
    "      #print(type(primals), type(tangents))\n",
    "      primals_out = jaxed(*primals)\n",
    "      as_np = [np.asarray(prim) for prim in primals]\n",
    "      grads = [jnp.array(grad_fns[i](*as_np)) for i in argnums]\n",
    "\n",
    "      # if len(tangents) > len(grads):\n",
    "      #   raise RuntimeError(\"passed `num_args` is lower than the actual number of arguments\")\n",
    "      \n",
    "      contributions = jnp.array([jnp.dot(grads[i],tangents[i].flatten()) for i in argnums])\n",
    "  \n",
    "      return primals_out, jnp.sum(contributions)\n",
    "    return jaxed\n",
    "  return inner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
