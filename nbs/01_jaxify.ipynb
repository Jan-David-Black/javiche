{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jaxify\n",
    "\n",
    "> The core functionality of `javiche`. Defines the `@jaxit()` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp jaxify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import numpy as np\n",
    "import jax\n",
    "import autograd\n",
    "import jax.numpy as jnp\n",
    "from typing import List\n",
    "from functools import lru_cache, wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# from https://gist.github.com/Susensio/61f4fee01150caaac1e10fc5f005eb75\n",
    "def np_cache(*args, **kwargs): \n",
    "    \"\"\"LRU cache implementation for functions whose FIRST parameter is a numpy array\n",
    "    >>> array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    >>> @np_cache(maxsize=256)\n",
    "    ... def multiply(array, factor):\n",
    "    ...     print(\"Calculating...\")\n",
    "    ...     return factor*array\n",
    "    >>> multiply(array, 2)\n",
    "    Calculating...\n",
    "    array([[ 2,  4,  6],\n",
    "           [ 8, 10, 12]])\n",
    "    >>> multiply(array, 2)\n",
    "    array([[ 2,  4,  6],\n",
    "           [ 8, 10, 12]])\n",
    "    >>> multiply.cache_info()\n",
    "    CacheInfo(hits=1, misses=1, maxsize=256, currsize=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    def decorator(function):\n",
    "        @wraps(function)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            mod_args = []\n",
    "            for i,arg in enumerate(args): # modified to allow arbitrary amounts of arrays\n",
    "              if isinstance(arg, np.ndarray) or isinstance(arg, jax.Array):\n",
    "                mod_args.append(array_to_tuple(arg))\n",
    "              else:\n",
    "                mod_args.append(arg)\n",
    "            return cached_wrapper(*mod_args, **kwargs)\n",
    "\n",
    "        @lru_cache(*args, **kwargs)\n",
    "        def cached_wrapper(hashable_array, *args, **kwargs):\n",
    "            array = np.array(hashable_array)\n",
    "            return function(array, *args, **kwargs)\n",
    "\n",
    "        def array_to_tuple(np_array):\n",
    "            \"\"\"Iterates recursivelly.\"\"\"\n",
    "            #print(type(np_array))\n",
    "            if isinstance(np_array, jax.Array):\n",
    "              np_array = np.asarray(np_array)\n",
    "            try:\n",
    "              return tuple(array_to_tuple(_) for _ in np_array)\n",
    "            except TypeError:\n",
    "              return np_array\n",
    "\n",
    "        # copy lru_cache attributes over too\n",
    "        wrapper.cache_info = cached_wrapper.cache_info\n",
    "        wrapper.cache_clear = cached_wrapper.cache_clear\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def as_numpy(x):\n",
    "  def as_numpy_map(a):\n",
    "    if isinstance(a, jnp.ndarray):\n",
    "      return np.asarray(a)\n",
    "    else:\n",
    "      return a\n",
    "  return jax.tree_util.tree_map(as_numpy_map, x)\n",
    "\n",
    "\n",
    "def as_jax(x):\n",
    "  def as_jax_map(a):\n",
    "    if isinstance(a, np.ndarray):\n",
    "      return jnp.asarray(a)\n",
    "    else:\n",
    "      return a\n",
    "  return jax.tree_util.tree_map(as_jax_map, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def jaxit(\n",
    "  cache: bool = False\n",
    "  ):\n",
    "  \"\"\"\n",
    "    make a function that internally uses autograd compatible to jax gradient calculations\n",
    "\n",
    "    Attention: only a single output variable is supported\n",
    "  \"\"\"\n",
    "  def identity_decorator(fn):\n",
    "    return fn\n",
    "  \n",
    "  caching_decorator = identity_decorator\n",
    "  if cache:\n",
    "    caching_decorator = np_cache()\n",
    "\n",
    "  def inner(function):\n",
    "\n",
    "    @jax.custom_vjp\n",
    "    @caching_decorator\n",
    "    def jaxed(*args):\n",
    "      return as_jax(function(*as_numpy(args)))\n",
    "\n",
    "    @caching_decorator\n",
    "    def f_fwd(*args):\n",
    "      args = as_numpy(args)\n",
    "      argnums = tuple(i for i, _ in enumerate(args))\n",
    "\n",
    "      def f_ag_tupled(*args):\n",
    "        ans = function(*args)\n",
    "        if isinstance(ans, tuple):\n",
    "          return autograd.builtins.tuple(ans)\n",
    "        else:\n",
    "          return ans\n",
    "\n",
    "      vjp_f, ans = autograd.make_vjp(f_ag_tupled, argnums)(*args)\n",
    "      return as_jax(ans), jax.tree_util.Partial(vjp_f)\n",
    "\n",
    "\n",
    "    def f_rev(vjp_f, g):\n",
    "      g = as_numpy(g)\n",
    "      return as_jax(vjp_f(g))\n",
    "\n",
    "\n",
    "    jaxed.defvjp(f_fwd, f_rev)\n",
    "\n",
    "    return jaxed\n",
    "  return inner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "y",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
